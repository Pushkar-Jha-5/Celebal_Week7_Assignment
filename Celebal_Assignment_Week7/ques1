--Let's suppose you have 3 different types of file 1) CUST_MSTR_20191112.csv 2) master_child_export-20191112.csv 3) H_ECOM_ORDER.csv All these files will be in the data lake container You have to fetch all three types of files into their respective folders. Note: There could be multiple files on all 3 types for different dates for example CUST_MSTR_20191112.csv and CUST_MSTR_20191113.csv 1) For the "CUST_MSTR" starting name of the file You have to create an additional column for a date that will fetch the data value from the filename and put it into an additional column Date format: 2019-11-12 and load it into the "CUST_MSTR" table 2) For the "master_child_export" starting name of the file You have to create two additional columns date and date key which will fetch the data from the filename and put it into the additional columns. Date format: 2019-11-12 DateKey format: 20191112 and load it into the "master_child" table 3) for the "H_ECOM_ORDER" type of file you have to load it into the database as it is. and load it into "H_ECOM_Orders" table Note: This process will work on truncate load on a daily basis--

--Project Structure--
daily_csv_loader/
├── data_loader.py
├── config.py
├── requirements.txt
└── README.md

--config.py (for DB and file config)--
# config.py

DATABASE_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'your_database_name',
    'user': 'your_username',
    'password': 'your_password'
}

DATA_DIRECTORY = '/path/to/data/lake/folder'

--data_loader.py--
# data_loader.py
# __define-ocg__ CSV Loader for Daily Truncate and Load

import os
import pandas as pd
from datetime import datetime
import psycopg2
from config import DATABASE_CONFIG, DATA_DIRECTORY

def extract_date(filename, sep='_', date_format='%Y%m%d'):
    """Extract date from filename."""
    date_part = ''.join(filter(str.isdigit, filename.split(sep)[-1].split('.')[0]))
    return datetime.strptime(date_part, '%Y%m%d')

def connect_db():
    return psycopg2.connect(**DATABASE_CONFIG)

def truncate_and_load(df, table_name, conn):
    with conn.cursor() as cursor:
        cursor.execute(f"TRUNCATE TABLE {table_name}")
        conn.commit()
        # Creating insert query dynamically
        cols = ','.join(df.columns)
        vals = ','.join(['%s'] * len(df.columns))
        insert_query = f"INSERT INTO {table_name} ({cols}) VALUES ({vals})"
        cursor.executemany(insert_query, df.values.tolist())
        conn.commit()

def process_files():
    conn = connect_db()
    for file in os.listdir(DATA_DIRECTORY):
        filepath = os.path.join(DATA_DIRECTORY, file)
        if file.startswith("CUST_MSTR_") and file.endswith(".csv"):
            df = pd.read_csv(filepath)
            date = extract_date(file)
            df['file_date'] = date.strftime('%Y-%m-%d')
            truncate_and_load(df, 'CUST_MSTR', conn)
            print(f"Loaded: {file} into CUST_MSTR")

        elif file.startswith("master_child_export-") and file.endswith(".csv"):
            df = pd.read_csv(filepath)
            date = extract_date(file, sep='-')
            df['file_date'] = date.strftime('%Y-%m-%d')
            df['file_date_key'] = date.strftime('%Y%m%d')
            truncate_and_load(df, 'master_child', conn)
            print(f"Loaded: {file} into master_child")

        elif file.startswith("H_ECOM_ORDER") and file.endswith(".csv"):
            df = pd.read_csv(filepath)
            truncate_and_load(df, 'H_ECOM_Orders', conn)
            print(f"Loaded: {file} into H_ECOM_Orders")
    conn.close()

if __name__ == "__main__":
    process_files()

--requirements.txt--
pandas
psycopg2

--Example Folder Structure--
/data_lake/
├── CUST_MSTR_20191112.csv
├── CUST_MSTR_20191113.csv
├── master_child_export-20191112.csv
├── H_ECOM_ORDER.csv
